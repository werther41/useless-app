{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NER + TF-IDF Topic Extraction Backfill\n",
        "\n",
        "This notebook processes existing news articles to extract topics using Named Entity Recognition (NER) and TF-IDF scoring.\n",
        "\n",
        "## Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "from dotenv import load_dotenv\n",
        "import libsql_client\n",
        "import google.generativeai as genai\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "TURSO_URL = os.getenv(\"TURSO_DATABASE_URL\")\n",
        "TURSO_TOKEN = os.getenv(\"TURSO_AUTH_TOKEN\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Processing configuration\n",
        "BATCH_SIZE = 10\n",
        "MAX_ARTICLES = None  # None = process all\n",
        "DAYS_BACK = 7  # None = all time\n",
        "DELAY_BETWEEN_CALLS = 1  # seconds\n",
        "DELAY_BETWEEN_BATCHES = 3  # seconds\n",
        "\n",
        "# Initialize clients\n",
        "client = libsql_client.create_client(\n",
        "    url=TURSO_URL,\n",
        "    auth_token=TURSO_TOKEN\n",
        ")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
        "\n",
        "print(\"✅ Configuration loaded\")\n",
        "print(f\"Database: {TURSO_URL[:50]}...\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Max articles: {MAX_ARTICLES or 'All'}\")\n",
        "print(f\"Days back: {DAYS_BACK or 'All time'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Database Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_articles_without_topics():\n",
        "    \"\"\"Get articles that don't have topics extracted yet\"\"\"\n",
        "    query = \"\"\"\n",
        "        SELECT na.* \n",
        "        FROM news_articles na\n",
        "        LEFT JOIN article_topics at ON na.id = at.article_id\n",
        "        WHERE at.id IS NULL\n",
        "    \"\"\"\n",
        "    \n",
        "    if DAYS_BACK:\n",
        "        query += f\" AND na.created_at >= datetime('now', '-{DAYS_BACK} days')\"\n",
        "    \n",
        "    query += \" ORDER BY na.created_at DESC\"\n",
        "    \n",
        "    if MAX_ARTICLES:\n",
        "        query += f\" LIMIT {MAX_ARTICLES}\"\n",
        "    \n",
        "    result = client.execute(query)\n",
        "    return result.rows\n",
        "\n",
        "def store_article_topics(article_id: str, topics: List[Dict]):\n",
        "    \"\"\"Store extracted topics for an article\"\"\"\n",
        "    for topic in topics:\n",
        "        topic_id = f\"topic_{int(time.time() * 1000)}_{os.urandom(4).hex()}\"\n",
        "        \n",
        "        client.execute(\"\"\"\n",
        "            INSERT INTO article_topics (id, article_id, entity_text, entity_type, tfidf_score, ner_confidence)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", [\n",
        "            topic_id,\n",
        "            article_id,\n",
        "            topic['text'],\n",
        "            topic['type'],\n",
        "            topic.get('tfidf_score', 0.5),\n",
        "            topic.get('confidence', 0.8)\n",
        "        ])\n",
        "\n",
        "def update_trending_topics(topics: List[Dict]):\n",
        "    \"\"\"Update or create trending topics\"\"\"\n",
        "    for topic in topics:\n",
        "        # Check if topic exists\n",
        "        existing = client.execute(\"\"\"\n",
        "            SELECT id, occurrence_count, avg_tfidf_score, article_ids\n",
        "            FROM trending_topics\n",
        "            WHERE LOWER(topic_text) = LOWER(?)\n",
        "        \"\"\", [topic['text']])\n",
        "        \n",
        "        if existing.rows:\n",
        "            # Update existing topic\n",
        "            row = existing.rows[0]\n",
        "            new_score = (row['avg_tfidf_score'] + topic.get('tfidf_score', 0.5)) / 2\n",
        "            \n",
        "            client.execute(\"\"\"\n",
        "                UPDATE trending_topics\n",
        "                SET occurrence_count = occurrence_count + 1,\n",
        "                    avg_tfidf_score = ?,\n",
        "                    last_seen_at = CURRENT_TIMESTAMP\n",
        "                WHERE id = ?\n",
        "            \"\"\", [new_score, row['id']])\n",
        "        else:\n",
        "            # Create new trending topic\n",
        "            topic_id = f\"trending_{int(time.time() * 1000)}_{os.urandom(4).hex()}\"\n",
        "            \n",
        "            client.execute(\"\"\"\n",
        "                INSERT INTO trending_topics (id, topic_text, entity_type, occurrence_count, avg_tfidf_score, article_ids)\n",
        "                VALUES (?, ?, ?, ?, ?, ?)\n",
        "            \"\"\", [\n",
        "                topic_id,\n",
        "                topic['text'],\n",
        "                topic['type'],\n",
        "                1,\n",
        "                topic.get('tfidf_score', 0.5),\n",
        "                '[]'\n",
        "            ])\n",
        "\n",
        "print(\"✅ Database functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NER with Gemini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_topics_with_gemini(title: str, content: str) -> List[Dict]:\n",
        "    \"\"\"Extract topics using Gemini NER\"\"\"\n",
        "    prompt = f\"\"\"Extract 5-10 key topics/entities from this news article. Focus on:\n",
        "- Organizations, companies, technologies\n",
        "- People, scientists, researchers\n",
        "- Locations, countries, regions  \n",
        "- Scientific concepts, discoveries\n",
        "- Events, phenomena\n",
        "\n",
        "Title: {title}\n",
        "Content: {content[:1000]} \n",
        "\n",
        "Return ONLY a JSON array with this format:\n",
        "[{{\"text\": \"entity name\", \"type\": \"TECH|ORG|PERSON|LOCATION|CONCEPT|EVENT\"}}]\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0.3,\n",
        "                max_output_tokens=1000,\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # Extract JSON from response\n",
        "        text = response.text\n",
        "        json_start = text.find('[')\n",
        "        json_end = text.rfind(']') + 1\n",
        "        \n",
        "        if json_start >= 0 and json_end > json_start:\n",
        "            topics = json.loads(text[json_start:json_end])\n",
        "            return [\n",
        "                {\n",
        "                    'text': t['text'],\n",
        "                    'type': t.get('type', 'CONCEPT'),\n",
        "                    'confidence': 0.8,\n",
        "                    'tfidf_score': 0.5  # Will be updated with TF-IDF\n",
        "                }\n",
        "                for t in topics\n",
        "            ]\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting topics: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test with one article\n",
        "articles = get_articles_without_topics()\n",
        "if len(articles) > 0:\n",
        "    test_article = articles[0]\n",
        "    test_topics = extract_topics_with_gemini(test_article['title'], test_article['content'])\n",
        "    print(f\"\\n🧪 Test extraction for: {test_article['title'][:60]}...\")\n",
        "    print(f\"Found {len(test_topics)} topics:\")\n",
        "    for topic in test_topics[:5]:\n",
        "        print(f\"  - {topic['text']} ({topic['type']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_tfidf_scores(articles: List[Dict]) -> Dict[str, float]:\n",
        "    \"\"\"Calculate TF-IDF scores for all terms across articles\"\"\"\n",
        "    # Prepare documents\n",
        "    documents = []\n",
        "    for article in articles:\n",
        "        doc = f\"{article['title']} {article['content']}\"\n",
        "        documents.append(doc)\n",
        "    \n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=1000,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2),  # Include bigrams\n",
        "        min_df=2,  # Term must appear in at least 2 documents\n",
        "        max_df=0.8  # Term must appear in less than 80% of documents\n",
        "    )\n",
        "    \n",
        "    # Fit and transform\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    # Calculate average TF-IDF scores\n",
        "    mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
        "    \n",
        "    # Create mapping of terms to scores\n",
        "    term_scores = dict(zip(feature_names, mean_scores))\n",
        "    \n",
        "    return term_scores\n",
        "\n",
        "def update_topic_tfidf_scores(topics: List[Dict], tfidf_scores: Dict[str, float]):\n",
        "    \"\"\"Update topics with TF-IDF scores\"\"\"\n",
        "    for topic in topics:\n",
        "        # Find best matching term in TF-IDF scores\n",
        "        topic_text = topic['text'].lower()\n",
        "        best_score = 0.0\n",
        "        best_term = None\n",
        "        \n",
        "        for term, score in tfidf_scores.items():\n",
        "            if topic_text in term or term in topic_text:\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_term = term\n",
        "        \n",
        "        if best_term:\n",
        "            topic['tfidf_score'] = best_score\n",
        "            topic['matched_term'] = best_term\n",
        "        else:\n",
        "            topic['tfidf_score'] = 0.1  # Default low score\n",
        "\n",
        "print(\"✅ TF-IDF functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Processing Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all articles without topics\n",
        "articles = get_articles_without_topics()\n",
        "print(f\"📊 Found {len(articles)} articles without topics\")\n",
        "\n",
        "if len(articles) == 0:\n",
        "    print(\"🎉 All articles already have topics!\")\n",
        "else:\n",
        "    # Calculate TF-IDF scores for all articles\n",
        "    print(\"\\n🔢 Calculating TF-IDF scores...\")\n",
        "    tfidf_scores = calculate_tfidf_scores(articles)\n",
        "    print(f\"✅ Calculated TF-IDF for {len(tfidf_scores)} terms\")\n",
        "    \n",
        "    # Process in batches\n",
        "    total_processed = 0\n",
        "    total_errors = 0\n",
        "    all_errors = []\n",
        "    \n",
        "    print(f\"\\n🚀 Starting processing of {len(articles)} articles...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i in range(0, len(articles), BATCH_SIZE):\n",
        "        batch = articles[i:i + BATCH_SIZE]\n",
        "        batch_num = (i // BATCH_SIZE) + 1\n",
        "        total_batches = (len(articles) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "        \n",
        "        print(f\"\\n📦 Processing batch {batch_num}/{total_batches} ({len(batch)} articles)\")\n",
        "        \n",
        "        for j, article in enumerate(batch):\n",
        "            try:\n",
        "                print(f\"\\n[{j+1}/{len(batch)}] Processing: {article['title'][:60]}...\")\n",
        "                \n",
        "                # Extract topics\n",
        "                topics = extract_topics_with_gemini(article['title'], article['content'])\n",
        "                \n",
        "                if not topics:\n",
        "                    print(f\"  ⚠️  No topics extracted\")\n",
        "                    all_errors.append(f\"{article['id']}: No topics extracted\")\n",
        "                    total_errors += 1\n",
        "                    continue\n",
        "                \n",
        "                # Update with TF-IDF scores\n",
        "                update_topic_tfidf_scores(topics, tfidf_scores)\n",
        "                \n",
        "                # Store topics\n",
        "                store_article_topics(article['id'], topics)\n",
        "                update_trending_topics(topics)\n",
        "                \n",
        "                total_processed += 1\n",
        "                print(f\"  ✅ Extracted {len(topics)} topics\")\n",
        "                print(f\"     Topics: {', '.join([t['text'] for t in topics[:3]])}...\")\n",
        "                \n",
        "                # Rate limiting\n",
        "                time.sleep(DELAY_BETWEEN_CALLS)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Error: {e}\")\n",
        "                all_errors.append(f\"{article['id']}: {str(e)}\")\n",
        "                total_errors += 1\n",
        "        \n",
        "        print(f\"✅ Batch {batch_num} complete: {len(batch)} articles processed\")\n",
        "        \n",
        "        # Delay between batches\n",
        "        if i + BATCH_SIZE < len(articles):\n",
        "            print(f\"⏸️  Waiting {DELAY_BETWEEN_BATCHES}s before next batch...\")\n",
        "            time.sleep(DELAY_BETWEEN_BATCHES)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"🎉 Processing complete!\")\n",
        "    print(f\"✅ Total processed: {total_processed}\")\n",
        "    print(f\"❌ Total errors: {total_errors}\")\n",
        "    \n",
        "    if all_errors:\n",
        "        print(f\"\\nError details (first 10):\")\n",
        "        for error in all_errors[:10]:\n",
        "            print(f\"  - {error}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get final statistics\n",
        "stats = client.execute(\"\"\"\n",
        "    SELECT \n",
        "        COUNT(DISTINCT na.id) as total_articles,\n",
        "        COUNT(DISTINCT at.article_id) as articles_with_topics,\n",
        "        COUNT(DISTINCT tt.id) as total_trending_topics\n",
        "    FROM news_articles na\n",
        "    LEFT JOIN article_topics at ON na.id = at.article_id\n",
        "    LEFT JOIN trending_topics tt ON 1=1\n",
        "\"\"\")\n",
        "\n",
        "row = stats.rows[0]\n",
        "coverage = (row['articles_with_topics'] / row['total_articles'] * 100) if row['total_articles'] > 0 else 0\n",
        "\n",
        "print(\"\\n📊 Final Statistics:\")\n",
        "print(f\"Total articles: {row['total_articles']}\")\n",
        "print(f\"Articles with topics: {row['articles_with_topics']}\")\n",
        "print(f\"Articles without topics: {row['total_articles'] - row['articles_with_topics']}\")\n",
        "print(f\"Total trending topics: {row['total_trending_topics']}\")\n",
        "print(f\"Coverage: {coverage:.2f}%\")\n",
        "\n",
        "# Show top trending topics\n",
        "top_topics = client.execute(\"\"\"\n",
        "    SELECT topic_text, entity_type, occurrence_count, avg_tfidf_score\n",
        "    FROM trending_topics\n",
        "    ORDER BY (avg_tfidf_score * occurrence_count) DESC\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\\n🔥 Top 10 Trending Topics:\")\n",
        "for i, topic in enumerate(top_topics.rows, 1):\n",
        "    print(f\"{i:2d}. {topic['topic_text']} ({topic['entity_type']}) - \"\n",
        "          f\"Count: {topic['occurrence_count']}, Score: {topic['avg_tfidf_score']:.3f}\")\n",
        "\n",
        "# Show topic distribution by type\n",
        "type_dist = client.execute(\"\"\"\n",
        "    SELECT entity_type, COUNT(*) as count\n",
        "    FROM trending_topics\n",
        "    GROUP BY entity_type\n",
        "    ORDER BY count DESC\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\\n📈 Topic Distribution by Type:\")\n",
        "for row in type_dist.rows:\n",
        "    print(f\"  {row['entity_type']}: {row['count']} topics\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
