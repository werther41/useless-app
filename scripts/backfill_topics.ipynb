{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER + TF-IDF Topic Extraction Backfill\n",
    "\n",
    "This notebook processes existing news articles to extract topics using Named Entity Recognition (NER) with Google Gemini and TF-IDF scoring.\n",
    "\n",
    "## Overview\n",
    "1. Load articles from database\n",
    "2. Extract entities using Gemini NER\n",
    "3. Calculate TF-IDF scores\n",
    "4. Store topics and update trending topics\n",
    "5. Generate statistics and visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER + TF-IDF Topic Extraction Backfill\n",
    "\n",
    "This notebook processes existing news articles to extract topics using Named Entity Recognition (NER) with Google Gemini and TF-IDF scoring.\n",
    "\n",
    "## Overview\n",
    "1. Load articles from database\n",
    "2. Extract entities using Gemini NER\n",
    "3. Calculate TF-IDF scores\n",
    "4. Store topics and update trending topics\n",
    "5. Generate statistics and visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Guard heavy plotting libs to avoid NumPy ABI issues in headless runs\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "except Exception as e:\n",
    "    plt = None\n",
    "    sns = None\n",
    "    print(f\"‚ö†Ô∏è Skipping matplotlib/seaborn due to import issue: {e}\")\n",
    "\n",
    "# Database and AI imports\n",
    "import libsql_client\n",
    "import google.generativeai as genai\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful (plotting libs optional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 15  # Process articles in batches\n",
    "RATE_LIMIT_DELAY = 2  # Seconds between API calls\n",
    "MAX_ARTICLES = None  # Set to None for all articles, or number to limit\n",
    "RUN_PROCESSING = True  # Safety: do not run heavy processing by default\n",
    "\n",
    "# Entity types for NER (updated)\n",
    "ENTITY_TYPES = [\n",
    "    'PERSON', 'ORG', 'LOCATION', 'PRODUCT', 'PROGRAMMING_LANGUAGE', \n",
    "    'SCIENTIFIC_TERM', 'FIELD_OF_STUDY', 'EVENT', 'WORK_OF_ART', 'LAW_OR_POLICY'\n",
    "]\n",
    "\n",
    "# TF-IDF configuration\n",
    "TFIDF_CONFIG = {\n",
    "    'max_features': 1000,\n",
    "    'stop_words': 'english',\n",
    "    'ngram_range': (1, 2),  # unigrams and bigrams\n",
    "    'min_df': 2,  # minimum document frequency\n",
    "    'sublinear_tf': True  # log scaling\n",
    "}\n",
    "\n",
    "print(f\"üìä Configuration loaded:\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Rate limit: {RATE_LIMIT_DELAY}s\")\n",
    "print(f\"   Max articles: {MAX_ARTICLES or 'All'}\")\n",
    "print(f\"   Entity types: {ENTITY_TYPES}\")\n",
    "print(f\"   Run processing: {RUN_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database connection\n",
    "database_url = os.getenv('TURSO_DATABASE_URL')\n",
    "auth_token = os.getenv('TURSO_AUTH_TOKEN')\n",
    "\n",
    "if not database_url or not auth_token:\n",
    "    raise ValueError(\"Missing TURSO_DATABASE_URL or TURSO_AUTH_TOKEN in environment\")\n",
    "\n",
    "# Force HTTPS for cloud Turso (avoid wss)\n",
    "http_url = database_url.replace('libsql', 'https', 1)\n",
    "\n",
    "# Create database client\n",
    "client = libsql_client.create_client(\n",
    "    url=http_url,\n",
    "    auth_token=auth_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Database connection established (HTTPS)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test database connection (Fixed - using await)\n",
    "try:\n",
    "    result = await client.execute(\"SELECT COUNT(*) FROM news_articles\")\n",
    "    total_articles = result.rows[0][0]\n",
    "    print(f\"üì∞ Found {total_articles} articles in database\")\n",
    "    \n",
    "    # Check existing topics\n",
    "    result = await client.execute(\"SELECT COUNT(*) FROM article_topics\")\n",
    "    existing_topics = result.rows[0][0]\n",
    "    print(f\"üè∑Ô∏è  Found {existing_topics} existing topics\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database test failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database helper functions (Fixed - using async/await)\n",
    "\n",
    "async def get_articles_without_topics():\n",
    "    \"\"\"Get articles that don't have topics extracted yet\"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT na.* \n",
    "        FROM news_articles na\n",
    "        LEFT JOIN article_topics at ON na.id = at.article_id\n",
    "        WHERE at.id IS NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    if MAX_ARTICLES:\n",
    "        query += f\" LIMIT {MAX_ARTICLES}\"\n",
    "    \n",
    "    result = await client.execute(query)\n",
    "    return result.rows\n",
    "\n",
    "\n",
    "async def store_article_topics(article_id: str, topics: List[Dict]):\n",
    "    \"\"\"Store extracted topics for an article\"\"\"\n",
    "    for topic in topics:\n",
    "        topic_id = f\"topic_{int(time.time() * 1000)}_{os.urandom(4).hex()}\"\n",
    "        \n",
    "        await client.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO article_topics (id, article_id, entity_text, entity_type, tfidf_score, ner_confidence)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            [\n",
    "                topic_id,\n",
    "                article_id,\n",
    "                topic['text'],\n",
    "                topic['type'],\n",
    "                topic.get('tfidf_score', 0.5),\n",
    "                topic.get('confidence', 0.8)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "async def update_trending_topics(topics: List[Dict]):\n",
    "    \"\"\"Update or create trending topics\"\"\"\n",
    "    for topic in topics:\n",
    "        # Check if topic exists\n",
    "        existing = await client.execute(\n",
    "            \"\"\"\n",
    "            SELECT id, occurrence_count, avg_tfidf_score, article_ids\n",
    "            FROM trending_topics\n",
    "            WHERE LOWER(topic_text) = LOWER(?)\n",
    "            \"\"\",\n",
    "            [topic['text']],\n",
    "        )\n",
    "        \n",
    "        if existing.rows:\n",
    "            # Update existing topic\n",
    "            row = existing.rows[0]\n",
    "            new_score = (row['avg_tfidf_score'] + topic.get('tfidf_score', 0.5)) / 2\n",
    "            \n",
    "            await client.execute(\n",
    "                \"\"\"\n",
    "                UPDATE trending_topics\n",
    "                SET occurrence_count = occurrence_count + 1,\n",
    "                    avg_tfidf_score = ?,\n",
    "                    last_seen_at = CURRENT_TIMESTAMP\n",
    "                WHERE id = ?\n",
    "                \"\"\",\n",
    "                [new_score, row['id']],\n",
    "            )\n",
    "        else:\n",
    "            # Create new trending topic\n",
    "            topic_id = f\"trending_{int(time.time() * 1000)}_{os.urandom(4).hex()}\"\n",
    "            \n",
    "            await client.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO trending_topics (id, topic_text, entity_type, occurrence_count, avg_tfidf_score, article_ids)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                [\n",
    "                    topic_id,\n",
    "                    topic['text'],\n",
    "                    topic['type'],\n",
    "                    1,\n",
    "                    topic.get('tfidf_score', 0.5),\n",
    "                    '[]',\n",
    "                ],\n",
    "            )\n",
    "\n",
    "print(\"‚úÖ Database helper functions ready (async/await)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER with Gemini\n",
    "\n",
    "def build_entity_extraction_prompt(title: str, content: str) -> str:\n",
    "    \"\"\"Build the entity extraction prompt\"\"\"\n",
    "    return f\"\"\"You are an expert NLP system. Your task is to extract the 5-10 most important named entities and key concepts from the following news article.\n",
    "\n",
    "Focus on identifying specific and relevant items. Use the following entity types:\n",
    "- **PERSON**: People, scientists, researchers.\n",
    "- **ORG**: Organizations, companies, institutions (e.g., \"NASA\", \"Google\").\n",
    "- **LOCATION**: Geographical places, countries, cities.\n",
    "- **PRODUCT**: Specific software, hardware, or services (e.g., \"iPhone 17\", \"GitHub Copilot\").\n",
    "- **PROGRAMMING_LANGUAGE**: Programming languages (e.g., \"Python\", \"Rust\").\n",
    "- **SCIENTIFIC_TERM**: Specific scientific concepts, theories, species, or astronomical bodies (e.g., \"black hole\", \"CRISPR\").\n",
    "- **FIELD_OF_STUDY**: Broader domains of knowledge (e.g., \"Machine Learning\", \"Astrophysics\").\n",
    "- **EVENT**: Specific named events, conferences, or historical periods (e.g., \"WWDC 2025\", \"The Renaissance\").\n",
    "- **WORK_OF_ART**: Named creative works like books, films, or paintings.\n",
    "- **LAW_OR_POLICY**: Named laws, regulations, or policies (e.g., \"GDPR\").\n",
    "\n",
    "**Article to Analyze:**\n",
    "Title: {title}\n",
    "Content: {content[:1500]}  # Increased character limit slightly for better context\n",
    "\n",
    "**Instructions:**\n",
    "1.  Analyze the title and content to find the most significant topics.\n",
    "2.  Do not extract generic or overly broad terms (e.g., \"science\", \"research\").\n",
    "3.  Return **ONLY** a raw JSON array with the specified format. Do not add any introductory text, explanations, or markdown formatting like ```json.\n",
    "\n",
    "**JSON Output Format:**\n",
    "[\n",
    "  {{\"text\": \"entity name\", \"type\": \"TYPE_FROM_LIST_ABOVE\"}},\n",
    "  {{\"text\": \"another entity\", \"type\": \"TYPE_FROM_LIST_ABOVE\"}}\n",
    "]\"\"\"\n",
    "\n",
    "\n",
    "def extract_topics_with_gemini(title: str, content: str) -> List[Dict]:\n",
    "    \"\"\"Extract topics using Gemini NER with new prompt and types\"\"\"\n",
    "    prompt = build_entity_extraction_prompt(title, content)\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=1000,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        text = response.text\n",
    "        json_start = text.find('[')\n",
    "        json_end = text.rfind(']') + 1\n",
    "        \n",
    "        if json_start >= 0 and json_end > json_start:\n",
    "            topics = json.loads(text[json_start:json_end])\n",
    "            return [\n",
    "                {\n",
    "                    'text': t['text'],\n",
    "                    'type': t.get('type', 'SCIENTIFIC_TERM'),\n",
    "                    'confidence': 0.8,\n",
    "                    'tfidf_score': 0.5  # Will be updated with TF-IDF\n",
    "                }\n",
    "                for t in topics\n",
    "            ]\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting topics: {e}\")\n",
    "        return []\n",
    "\n",
    "# Configure Gemini (guard)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if GOOGLE_API_KEY:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "    print(\"‚úÖ Gemini configured\")\n",
    "else:\n",
    "    model = None\n",
    "    print(\"‚ö†Ô∏è GOOGLE_API_KEY not set; Gemini NER disabled\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF utilities\n",
    "\n",
    "def calculate_tfidf_scores(articles: List[Dict]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate TF-IDF scores for all terms across articles\"\"\"\n",
    "    # Prepare documents\n",
    "    documents = []\n",
    "    for article in articles:\n",
    "        doc = f\"{article['title']} {article['content']}\"\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=TFIDF_CONFIG['max_features'],\n",
    "        stop_words=TFIDF_CONFIG['stop_words'],\n",
    "        ngram_range=TFIDF_CONFIG['ngram_range'],\n",
    "        min_df=TFIDF_CONFIG['min_df'],\n",
    "        sublinear_tf=TFIDF_CONFIG['sublinear_tf'],\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate average TF-IDF scores\n",
    "    mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "    \n",
    "    # Create mapping of terms to scores\n",
    "    term_scores = dict(zip(feature_names, mean_scores))\n",
    "    \n",
    "    return term_scores\n",
    "\n",
    "\n",
    "def update_topic_tfidf_scores(topics: List[Dict], tfidf_scores: Dict[str, float]):\n",
    "    \"\"\"Update topics with TF-IDF scores\"\"\"\n",
    "    for topic in topics:\n",
    "        # Find best matching term in TF-IDF scores\n",
    "        topic_text = topic['text'].lower()\n",
    "        best_score = 0.0\n",
    "        best_term = None\n",
    "        \n",
    "        for term, score in tfidf_scores.items():\n",
    "            if topic_text in term or term in topic_text:\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_term = term\n",
    "        \n",
    "        if best_term:\n",
    "            topic['tfidf_score'] = best_score\n",
    "            topic['matched_term'] = best_term\n",
    "        else:\n",
    "            topic['tfidf_score'] = 0.1  # Default low score\n",
    "\n",
    "print(\"‚úÖ TF-IDF utilities ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop (Fixed - using async/await)\n",
    "async def main_processing():\n",
    "    if RUN_PROCESSING:\n",
    "        # Get all articles without topics\n",
    "        articles = await get_articles_without_topics()\n",
    "        print(f\"üìä Found {len(articles)} articles without topics\")\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            print(\"üéâ All articles already have topics!\")\n",
    "        else:\n",
    "            # Calculate TF-IDF scores for all articles\n",
    "            print(\"\\nüî¢ Calculating TF-IDF scores...\")\n",
    "            tfidf_scores = calculate_tfidf_scores(articles)\n",
    "            print(f\"‚úÖ Calculated TF-IDF for {len(tfidf_scores)} terms\")\n",
    "            \n",
    "            total_processed = 0\n",
    "            total_errors = 0\n",
    "            all_errors = []\n",
    "            \n",
    "            print(f\"\\nüöÄ Starting processing of {len(articles)} articles...\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            for i in range(0, len(articles), BATCH_SIZE):\n",
    "                batch = articles[i:i + BATCH_SIZE]\n",
    "                batch_num = (i // BATCH_SIZE) + 1\n",
    "                total_batches = (len(articles) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "                \n",
    "                print(f\"\\nüì¶ Processing batch {batch_num}/{total_batches} ({len(batch)} articles)\")\n",
    "                \n",
    "                for j, article in enumerate(batch):\n",
    "                    try:\n",
    "                        print(f\"\\n[{j+1}/{len(batch)}] Processing: {article['title'][:60]}...\")\n",
    "                        \n",
    "                        if not model:\n",
    "                            raise RuntimeError(\"Gemini model not configured; set GOOGLE_API_KEY\")\n",
    "                        \n",
    "                        # Extract topics\n",
    "                        topics = extract_topics_with_gemini(article['title'], article['content'])\n",
    "                        \n",
    "                        if not topics:\n",
    "                            print(f\"  ‚ö†Ô∏è  No topics extracted\")\n",
    "                            all_errors.append(f\"{article['id']}: No topics extracted\")\n",
    "                            total_errors += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Update with TF-IDF scores\n",
    "                        update_topic_tfidf_scores(topics, tfidf_scores)\n",
    "                        \n",
    "                        # Store topics and update trending\n",
    "                        await store_article_topics(article['id'], topics)\n",
    "                        await update_trending_topics(topics)\n",
    "                        \n",
    "                        total_processed += 1\n",
    "                        print(f\"  ‚úÖ Extracted {len(topics)} topics\")\n",
    "                        print(f\"     Topics: {', '.join([t['text'] for t in topics[:3]])}...\")\n",
    "                        \n",
    "                        # Rate limiting\n",
    "                        time.sleep(RATE_LIMIT_DELAY)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Error: {e}\")\n",
    "                        all_errors.append(f\"{article['id']}: {str(e)}\")\n",
    "                        total_errors += 1\n",
    "                \n",
    "                print(f\"‚úÖ Batch {batch_num} complete: {len(batch)} articles processed\")\n",
    "                \n",
    "                # Delay between batches\n",
    "                if i + BATCH_SIZE < len(articles):\n",
    "                    print(f\"‚è∏Ô∏è  Waiting 3s before next batch...\")\n",
    "                    time.sleep(3)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"üéâ Processing complete!\")\n",
    "            print(f\"‚úÖ Total processed: {total_processed}\")\n",
    "            print(f\"‚ùå Total errors: {total_errors}\")\n",
    "            \n",
    "            if all_errors:\n",
    "                print(f\"\\nError details (first 10):\")\n",
    "                for error in all_errors[:10]:\n",
    "                    print(f\"  - {error}\")\n",
    "    else:\n",
    "        print(\"‚èπÔ∏è RUN_PROCESSING is False. Set to True to process articles.\")\n",
    "\n",
    "# Run the main processing (direct await in Jupyter)\n",
    "await main_processing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results and statistics (Fixed - using async/await)\n",
    "async def get_final_statistics():\n",
    "    # Basic coverage stats (safe to run)\n",
    "    stats = await client.execute(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT na.id) as total_articles,\n",
    "            COUNT(DISTINCT at.article_id) as articles_with_topics,\n",
    "            COUNT(DISTINCT tt.id) as total_trending_topics\n",
    "        FROM news_articles na\n",
    "        LEFT JOIN article_topics at ON na.id = at.article_id\n",
    "        LEFT JOIN trending_topics tt ON 1=1\n",
    "    \"\"\")\n",
    "\n",
    "    row = stats.rows[0]\n",
    "    coverage = (row['articles_with_topics'] / row['total_articles'] * 100) if row['total_articles'] > 0 else 0\n",
    "\n",
    "    print(\"\\nüìä Final Statistics:\")\n",
    "    print(f\"Total articles: {row['total_articles']}\")\n",
    "    print(f\"Articles with topics: {row['articles_with_topics']}\")\n",
    "    print(f\"Articles without topics: {row['total_articles'] - row['articles_with_topics']}\")\n",
    "    print(f\"Total trending topics: {row['total_trending_topics']}\")\n",
    "    print(f\"Coverage: {coverage:.2f}%\")\n",
    "\n",
    "    # Top trending topics (safe)\n",
    "    top_topics = await client.execute(\"\"\"\n",
    "        SELECT topic_text, entity_type, occurrence_count, avg_tfidf_score\n",
    "        FROM trending_topics\n",
    "        ORDER BY (avg_tfidf_score * occurrence_count) DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"\\nüî• Top 10 Trending Topics:\")\n",
    "    for i, topic in enumerate(top_topics.rows, 1):\n",
    "        print(f\"{i:2d}. {topic['topic_text']} ({topic['entity_type']}) - \"\n",
    "              f\"Count: {topic['occurrence_count']}, Score: {topic['avg_tfidf_score']:.3f}\")\n",
    "\n",
    "    # Optional plotting if libs available\n",
    "    if plt is not None and sns is not None:\n",
    "        type_dist = await client.execute(\"\"\"\n",
    "            SELECT entity_type, COUNT(*) as count\n",
    "            FROM trending_topics\n",
    "            GROUP BY entity_type\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        labels = [r['entity_type'] for r in type_dist.rows]\n",
    "        counts = [r['count'] for r in type_dist.rows]\n",
    "        \n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.barplot(x=labels, y=counts)\n",
    "        plt.title('Topic Distribution by Type')\n",
    "        plt.xlabel('Entity Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"(Plotting libraries unavailable; skipping charts)\")\n",
    "\n",
    "# Run the statistics (direct await in Jupyter)\n",
    "await get_final_statistics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
